{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_dir = \"/data/haocheng/data/bam/GM\"  # BAM 文件目录\n",
    "bam_files = os.listdir(bam_dir)  # 列出所有 BAM 文件\n",
    "chromosomes = [f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"]  # 选择的染色体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数：计算 BAM 文件的覆盖度\n",
    "def get_coverage(bam_file):\n",
    "    cmd = [\"samtools\", \"depth\", bam_file]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Normalization\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 m 和 n\n",
    "m = 2\n",
    "n = 1\n",
    "bam_file1 = os.path.join(bam_dir, bam_files[0])\n",
    "bam_file2 = os.path.join(bam_dir, bam_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage1 = get_coverage(bam_file1)\n",
    "coverage2 = get_coverage(bam_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将覆盖度数据转换为 DataFrame\n",
    "def create_df(coverage_data):\n",
    "    data = [line.split() for line in coverage_data.strip().split(\"\\n\")]\n",
    "    return spark.createDataFrame(data, schema=[\"chrom\", \"position\", \"coverage\"]).withColumn(\"coverage\", F.col(\"coverage\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage1_df = create_df(coverage1)\n",
    "coverage2_df = create_df(coverage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 函数：计算 BAM 文件的覆盖度并返回结果\n",
    "def get_coverage(bam_file):\n",
    "    cmd = [\"samtools\", \"depth\", bam_file]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout.strip().splitlines()\n",
    "\n",
    "# 获取目录下的所有 BAM 文件\n",
    "bam_dir = \"/data/haocheng/data/bam/GM/\"\n",
    "bam_files = [f for f in os.listdir(bam_dir) if f.endswith('.bam')]\n",
    "bam_files = sorted(bam_files)  # 确保文件按字母顺序排序\n",
    "\n",
    "# 初始化 m 和 n\n",
    "m = 2\n",
    "n = 1\n",
    "\n",
    "# 用于存储覆盖度的 DataFrame\n",
    "coverage_dfs = []\n",
    "\n",
    "# 处理前两个 BAM 文件\n",
    "for bam_file in bam_files[:2]:\n",
    "    bam_path = os.path.join(bam_dir, bam_file)\n",
    "    coverage = get_coverage(bam_path)\n",
    "    \n",
    "    # 将覆盖度数据添加到 DataFrame\n",
    "    df = pd.DataFrame([line.split() for line in coverage], columns=[\"chrom\", \"position\", \"coverage\"])\n",
    "    df[\"coverage\"] = df[\"coverage\"].astype(int)\n",
    "    coverage_dfs.append(df)\n",
    "\n",
    "# 计算每个覆盖度的最大值\n",
    "max_cov1 = coverage_dfs[0][\"coverage\"].max()\n",
    "max_cov2 = coverage_dfs[1][\"coverage\"].max()\n",
    "\n",
    "# 合并两个 DataFrame\n",
    "combined_coverage = pd.concat(coverage_dfs)\n",
    "combined_coverage = combined_coverage.groupby([\"chrom\", \"position\"], as_index=False).sum()\n",
    "\n",
    "# 归一化\n",
    "norm_max = max(max_cov1, max_cov2)\n",
    "combined_coverage['normalized_coverage'] = (combined_coverage['coverage'] / max_cov1 * norm_max * (n/m)) + \\\n",
    "                                            (combined_coverage['coverage'] / max_cov2 * norm_max * (1/m))\n",
    "\n",
    "# 输出结果\n",
    "output_file = \"/data/haocheng/data/bed/GM/normalized_coverage.bedgraph\"\n",
    "combined_coverage[['chrom', 'position', 'normalized_coverage']].to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "\n",
    "print(f\"所有 BAM 文件处理完成，结果将输出到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 函数：计算 BAM 文件的覆盖度并返回结果\n",
    "def get_coverage(bam_file):\n",
    "    cmd = [\"samtools\", \"depth\", bam_file]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout.strip().splitlines()\n",
    "\n",
    "# 获取目录下的所有 BAM 文件\n",
    "bam_dir = \"/data/haocheng/data/bam/GM/\"\n",
    "bam_files = [f for f in os.listdir(bam_dir) if f.endswith('.bam')]\n",
    "bam_files = sorted(bam_files)  # 确保文件按字母顺序排序\n",
    "\n",
    "# 初始化 m 和 n\n",
    "m = 2\n",
    "n = 1\n",
    "\n",
    "# 用于存储覆盖度的字典\n",
    "coverage_dict = {}\n",
    "\n",
    "# 处理前两个 BAM 文件\n",
    "for bam_file in bam_files[:2]:\n",
    "    bam_path = os.path.join(bam_dir, bam_file)\n",
    "    coverage = get_coverage(bam_path)\n",
    "    \n",
    "    for line in coverage:\n",
    "        chrom, position, cov = line.split()\n",
    "        position = int(position)\n",
    "        cov = int(cov)\n",
    "\n",
    "        # 使用字典来存储覆盖度数据\n",
    "        if (chrom, position) not in coverage_dict:\n",
    "            coverage_dict[(chrom, position)] = [0, 0]  # [cov1, cov2]\n",
    "        \n",
    "        # 根据文件索引更新覆盖度\n",
    "        if bam_file == bam_files[0]:\n",
    "            coverage_dict[(chrom, position)][0] += cov\n",
    "        else:\n",
    "            coverage_dict[(chrom, position)][1] += cov\n",
    "\n",
    "# 计算最大覆盖度\n",
    "max_cov1 = max(cov[0] for cov in coverage_dict.values())\n",
    "max_cov2 = max(cov[1] for cov in coverage_dict.values())\n",
    "\n",
    "# 归一化并输出结果\n",
    "norm_max = max(max_cov1, max_cov2)\n",
    "\n",
    "output_lines = []\n",
    "for (chrom, position), cov in coverage_dict.items():\n",
    "    norm_cov1 = (cov[0] / max_cov1 * norm_max * (n / m)) if max_cov1 > 0 else 0\n",
    "    norm_cov2 = (cov[1] / max_cov2 * norm_max * (1 / m)) if max_cov2 > 0 else 0\n",
    "    norm_cov = norm_cov1 + norm_cov2\n",
    "    output_lines.append(f\"{chrom}\\t{position}\\t{norm_cov}\")\n",
    "\n",
    "# 将结果写入文件\n",
    "output_file = \"/data/haocheng/data/bed/GM/normalized_coverage.bedgraph\"\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"所有 BAM 文件处理完成，结果将输出到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 计算最大覆盖度\n",
    "max_cov1 = cov1_rdd.map(lambda x: x[1]).max()\n",
    "max_cov2 = cov2_rdd.map(lambda x: x[1]).max()\n",
    "\n",
    "# 归一化\n",
    "norm_max = max(max_cov1, max_cov2)\n",
    "m = 2  # 根据你的需求设置\n",
    "n = 1  # 根据你的需求设置\n",
    "\n",
    "def normalize_coverage(record):\n",
    "    key, total_cov = record\n",
    "    chrom, position = key\n",
    "    norm1 = (total_cov / max_cov1) * norm_max * (n / m)\n",
    "    norm2 = (total_cov / max_cov2) * norm_max * (1 / m)\n",
    "    norm_cov = norm1 + norm2\n",
    "    return (chrom, position, norm_cov)\n",
    "\n",
    "normalized_rdd = combined_rdd.map(normalize_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并两个 DataFrame\n",
    "combined_df = coverage1_df.join(coverage2_df, on=[\"chrom\", \"position\"], how=\"outer\").fillna(0)\n",
    "# 计算最大覆盖度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算最大覆盖度\n",
    "max_cov1 = combined_df.agg(F.max(\"coverage\")).first()[0]\n",
    "max_cov2 = combined_df.agg(F.max(\"coverage\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "norm_max = max(max_cov1, max_cov2)\n",
    "m = 2  # 根据你的需求设置\n",
    "n = 1  # 根据你的需求设置\n",
    "normalized_df = combined_df.withColumn(\n",
    "    \"normalized_coverage\",\n",
    "    (F.col(\"coverage1\") / max_cov1 * norm_max * (n/m)) +\n",
    "    (F.col(\"coverage2\") / max_cov2 * norm_max * (1/m))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集结果到本地\n",
    "result = normalized_df.select(\"chrom\", \"position\", \"normalized_coverage\").rdd.map(lambda row: f\"{row.chrom}\\t{row.position}\\t{row.normalized_coverage}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "normalized_df.select(\"chrom\", \"position\", \"normalized_coverage\") \\\n",
    "    .write.csv(\"path/to/output/normalized_coverage.csv\", sep=\"\\t\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 关闭 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while bam_files:\n",
    "    next_bam = os.path.join(bam_dir, bam_files[0])\n",
    "    print(f\"现在正在处理 BAM 文件: {next_bam}\")\n",
    "\n",
    "    # 计算新的覆盖度\n",
    "    coverage2 = get_coverage(next_bam)\n",
    "\n",
    "    # 归一化并处理结果\n",
    "    print(\"归一化覆盖度...\")\n",
    "    result = normalize_with_spark(result, coverage2, 1000, m, n)\n",
    "\n",
    "    # 从数组中移除已使用的 BAM 文件\n",
    "    bam_files = bam_files[1:]  # 删除第一个元素\n",
    "    m += 1\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将最终结果写入文件\n",
    "output_file = \"/data/haocheng/data/bed/GM/GM_normalized_coverage.bedgraph\"\n",
    "\n",
    "print(f\"所有 BAM 文件处理完成，结果将输出到 {output_file}\")\n",
    "\n",
    "if result:\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"\\n\".join(result))\n",
    "    print(\"结果已成功写入文件。\")\n",
    "else:\n",
    "    print(\"结果为空，未写入文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_r_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
