{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "\n",
    "# 文件路径\n",
    "gtf_file = \"/data/haocheng/data/DNA/Homo_sapiens.GRCh38.104.gtf\"\n",
    "bigwig_file = \"/data/haocheng/data/bam/result/GM12878.bigwig\"\n",
    "expression_file = \"/data/haocheng/data/gene_expressiom/ENCFF345SHY.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genes found: 19924\n"
     ]
    }
   ],
   "source": [
    "genes = []\n",
    "\n",
    "# 允许使用不带 'chr' 前缀的染色体名称\n",
    "valid_chromosomes = [f'chr{i}' for i in range(1, 23)] + ['chrX', 'chrY'] + [str(i) for i in range(1, 23)] + ['X', 'Y']\n",
    "\n",
    "with open(gtf_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        fields = line.strip().split('\\t')\n",
    "        if fields[2] == 'gene':\n",
    "            info = {x.split(' ')[0]: x.split(' ')[1].strip('\";') for x in fields[8].split('; ')}\n",
    "            if info.get('gene_biotype') == 'protein_coding':\n",
    "                chrom = fields[0]\n",
    "                if chrom in valid_chromosomes:\n",
    "                    tss = int(fields[3])  # TSS是起始位置\n",
    "                    start = max(0, tss - 10000)  # 向前500个碱基\n",
    "                    end = tss + 10000  # 向后500个碱基\n",
    "                    gene_id = info['gene_id']\n",
    "                    genes.append([chrom, start, end, gene_id])\n",
    "                #else:\n",
    "                  # print(f\"Skipped non-chromosomal gene: {info['gene_id']} in {chrom}\")  # 仅打印非染色体基因的基因ID\n",
    "\n",
    "genes_df = pd.DataFrame(genes, columns=['chrom', 'start', 'end', 'gene_id'])\n",
    "\n",
    "# 仅在没有找到基因时打印\n",
    "if genes_df.empty:\n",
    "    print(\"No genes found.\")\n",
    "else:\n",
    "    print(f\"Total genes found: {len(genes_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!genome-build GRCh38.p13\n",
      "#!genome-version GRCh38\n",
      "#!genome-date 2013-12\n",
      "#!genome-build-accession GCA_000001405.28\n",
      "#!genebuild-last-updated 2021-03\n",
      "      chrom     start       end          gene_id\n",
      "0         1    675679    695679  ENSG00000284662\n",
      "1         1   1201340   1221340  ENSG00000186827\n",
      "2         1   1193508   1213508  ENSG00000186891\n",
      "3         1   1461765   1481765  ENSG00000160072\n",
      "4         1   6614866   6634866  ENSG00000041988\n",
      "...     ...       ...       ...              ...\n",
      "19919    21  34503142  34523142  ENSG00000159200\n",
      "19920    21  36146782  36166782  ENSG00000142197\n",
      "19921    21  15719982  15739982  ENSG00000155313\n",
      "19922    21   6489203   6509203  ENSG00000276076\n",
      "19923    21  31108416  31128416  ENSG00000156299\n",
      "\n",
      "[19924 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 查看 GTF 文件的前 5 行\n",
    "with open(gtf_file, 'r') as f:\n",
    "    for i in range(5):  # 打印前 5 行\n",
    "        print(f.readline().strip())\n",
    "print(genes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开 BigWig 文件\n",
    "bw = pyBigWig.open(bigwig_file)\n",
    "\n",
    "# 读取 BigWig 数据并添加到 DataFrame\n",
    "def get_bigwig_values(row):\n",
    "    chrom = f'chr{row[\"chrom\"]}'  # 将数字染色体转换为带 \"chr\" 前缀的格式\n",
    "    start = row['start']\n",
    "    end = row['end']\n",
    "    \n",
    "    # 获取 BigWig 信号值，返回长度为 1000 的值\n",
    "    values = bw.values(chrom, start, end)\n",
    "    \n",
    "    # 如果 BigWig 返回的值数量不足 1000，填充 0\n",
    "    if len(values) < 1000:\n",
    "        values.extend([0] * (1000 - len(values)))\n",
    "    \n",
    "    return values # 确保返回的是 1000 个值\n",
    "\n",
    "# 添加新列到 DataFrame\n",
    "genes_df['bigwig_values'] = genes_df.apply(get_bigwig_values, axis=1)\n",
    "\n",
    "# 关闭 BigWig 文件\n",
    "bw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenes_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'genes_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(genes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 TSV 文件\n",
    "expression_file = \"/data/haocheng/data/gene_expressiom/ENCFF345SHY.tsv\"\n",
    "expression_df = pd.read_csv(expression_file, sep='\\t')\n",
    "\n",
    "# 去掉 gene_id 中的版本号后缀\n",
    "expression_df['gene_id'] = expression_df['gene_id'].str.split('.').str[0]\n",
    "\n",
    "# 计算 log(TPM + 1)\n",
    "expression_df['log_TPM'] = np.log1p(expression_df['TPM'])\n",
    "\n",
    "# 进行合并\n",
    "merged_df = genes_df.merge(expression_df[['gene_id', 'log_TPM']], on='gene_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共同存在的 gene_id 数量: 19807\n"
     ]
    }
   ],
   "source": [
    "# 去掉 expression_df 中 gene_id 的小数点及其后面的部分\n",
    "expression_df['gene_id'] = expression_df['gene_id'].str.split('.').str[0]\n",
    "\n",
    "# 找到两个 DataFrame 中都存在的 gene_id\n",
    "common_ids = set(genes_df['gene_id']).intersection(set(expression_df['gene_id']))\n",
    "\n",
    "# 打印这些共同的 gene_id 及其数量\n",
    "print(f\"共同存在的 gene_id 数量: {len(common_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      chrom     start       end          gene_id  \\\n",
      "0         1    675679    695679  ENSG00000284662   \n",
      "1         1   1201340   1221340  ENSG00000186827   \n",
      "2         1   1193508   1213508  ENSG00000186891   \n",
      "3         1   1461765   1481765  ENSG00000160072   \n",
      "4         1   6614866   6634866  ENSG00000041988   \n",
      "...     ...       ...       ...              ...   \n",
      "19821    21  34503142  34523142  ENSG00000159200   \n",
      "19822    21  36146782  36166782  ENSG00000142197   \n",
      "19823    21  15719982  15739982  ENSG00000155313   \n",
      "19824    21   6489203   6509203  ENSG00000276076   \n",
      "19825    21  31108416  31128416  ENSG00000156299   \n",
      "\n",
      "                                           bigwig_values   log_TPM  \n",
      "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.058269  \n",
      "1      [1349.0, 1349.0, 1349.0, 1349.0, 1349.0, 1349....  1.261298  \n",
      "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.214913  \n",
      "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.730464  \n",
      "4      [160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 0.0...  1.736951  \n",
      "...                                                  ...       ...  \n",
      "19821  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.795087  \n",
      "19822  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.749200  \n",
      "19823  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.461297  \n",
      "19824  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.048790  \n",
      "19825  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.482426  \n",
      "\n",
      "[19826 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的大小: (18301, 6)\n",
      "测试集的大小: (1525, 6)\n",
      "Training set non values: 0\n",
      "Testing set non values: 0\n"
     ]
    }
   ],
   "source": [
    "# 假设 merged_df 是你之前合并后的数据\n",
    "# 选择要作为测试集的染色体\n",
    "test_chromosomes = ['13', '14', '15']  # 你可以根据需要修改这个列表\n",
    "\n",
    "# 划分训练集和测试集\n",
    "test_df = merged_df[merged_df['chrom'].isin(test_chromosomes)]\n",
    "train_df = merged_df[~merged_df['chrom'].isin(test_chromosomes)]\n",
    "\n",
    "# 打印结果\n",
    "print(\"训练集的大小:\", train_df.shape)\n",
    "print(\"测试集的大小:\", test_df.shape)\n",
    "# 检查训练集和测试集中的非数值（non）值数量\n",
    "non_train = train_df.isnull().sum().sum()  # 训练集中的非数值值总数\n",
    "non_test = test_df.isnull().sum().sum()  # 测试集中的非数值值总数\n",
    "\n",
    "print(f\"Training set non values: {non_train}\")\n",
    "print(f\"Testing set non values: {non_test}\")\n",
    "# 检查是否有无穷大或负值\n",
    "#print(np.isinf(X_train_scaled).sum())  # 检查无穷大值\n",
    "#print(np.isfinite(X_train_scaled).sum())  # 检查有限值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_df['bigwig_values'], bins=50)\n",
    "plt.title(\"Distribution of BigWig Values\")\n",
    "plt.xlabel(\"BigWig Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_df['log_TPM'], bins=50)\n",
    "plt.title(\"Distribution of log(TPM) Values\")\n",
    "plt.xlabel(\"log(TPM) Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自定义回调类，用于在每个周期结束时打印评估指标\n",
    "class CustomMetrics(Callback):\n",
    "    def __init__(self, X_test, y_test):\n",
    "        super().__init__()\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        pearson_corr, _ = pearsonr(self.y_test, y_pred.flatten())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: MSE = {mse:.4f}, MAE = {mae:.4f}, R² = {r2:.4f}, Pearson Correlation = {pearson_corr:.4f}\")\n",
    "\n",
    "# 假设 train_df 和 test_df 是你已经划分好的训练集和测试集\n",
    "\n",
    "# 数据预处理\n",
    "X_train = np.array(train_df['bigwig_values'].tolist())  # 特征：BigWig 信号值\n",
    "y_train = train_df['log_TPM'].values  # 目标：log(TPM)\n",
    "X_test = np.array(test_df['bigwig_values'].tolist())  # 测试集特征\n",
    "y_test = test_df['log_TPM'].values  # 测试集目标\n",
    "\n",
    "# 标准化输入特征\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # 对训练集进行标准化\n",
    "X_test_scaled = scaler.transform(X_test)  # 对测试集进行同样的标准化\n",
    "\n",
    "# Reshape 输入数据以符合 LSTM 的要求\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# 构建 LSTM 模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='tanh', input_shape=(X_train_scaled.shape[1], 1), return_sequences=True))  # 第一层 LSTM\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation='tanh'))  # 第二层 LSTM\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # 输出层\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 设置更小的学习率\n",
    "optimizer = Adam(learning_rate=0.0001)  # 调整学习率\n",
    "model.compile(optimizer = Adam(learning_rate=0.001, clipnorm=1.0), loss='mse')  # 使用 Adam 优化器和均方误差作为损失函数\n",
    "\n",
    "# 训练模型，使用自定义回调\n",
    "custom_metrics = CustomMetrics(X_test_scaled, y_test)\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[custom_metrics])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 7s 147ms/step\n",
      "Epoch 1: MSE = 2.4315, MAE = 1.2146, R² = -0.1177, Pearson Correlation = 0.2793, Accuracy = 0.0249\n",
      "458/458 [==============================] - 490s 1s/step - loss: 2.1622 - val_loss: 2.4161\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 7s 147ms/step\n",
      "Epoch 2: MSE = 2.2365, MAE = 1.1710, R² = -0.0280, Pearson Correlation = 0.3015, Accuracy = 0.0249\n",
      "458/458 [==============================] - 483s 1s/step - loss: 1.9532 - val_loss: 2.2332\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 7s 146ms/step\n",
      "Epoch 3: MSE = 2.2724, MAE = 1.1843, R² = -0.0446, Pearson Correlation = 0.2831, Accuracy = 0.0308\n",
      "458/458 [==============================] - 482s 1s/step - loss: 1.9092 - val_loss: 2.2633\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 7s 139ms/step\n",
      "Epoch 4: MSE = 2.2159, MAE = 1.1764, R² = -0.0186, Pearson Correlation = 0.2912, Accuracy = 0.0302\n",
      "458/458 [==============================] - 461s 1s/step - loss: 1.8918 - val_loss: 2.2203\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 7s 146ms/step\n",
      "Epoch 5: MSE = 2.3001, MAE = 1.1771, R² = -0.0573, Pearson Correlation = 0.2914, Accuracy = 0.0308\n",
      "458/458 [==============================] - 487s 1s/step - loss: 1.8684 - val_loss: 2.3029\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 7s 145ms/step\n",
      "Epoch 6: MSE = 2.3730, MAE = 1.1794, R² = -0.0908, Pearson Correlation = 0.2991, Accuracy = 0.0374\n",
      "458/458 [==============================] - 484s 1s/step - loss: 1.8549 - val_loss: 2.3956\n",
      "Epoch 7/100\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.8595"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 训练模型，使用自定义回调\u001b[39;00m\n\u001b[1;32m     76\u001b[0m custom_metrics \u001b[38;5;241m=\u001b[39m CustomMetrics(X_test_scaled, y_test)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Add, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 自定义回调类，用于在每个周期结束时打印评估指标\n",
    "class CustomMetrics(Callback):\n",
    "    def __init__(self, X_test, y_test, threshold=0.1):\n",
    "        super().__init__()\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        pearson_corr, _ = pearsonr(self.y_test, y_pred.flatten())\n",
    "        \n",
    "        accuracy = np.mean(np.abs(y_pred.flatten() - self.y_test) < self.threshold)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: MSE = {mse:.4f}, MAE = {mae:.4f}, R² = {r2:.4f}, Pearson Correlation = {pearson_corr:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# 数据预处理\n",
    "X_train = np.array(train_df['bigwig_values'].tolist())\n",
    "y_train = train_df['log_TPM'].values\n",
    "X_test = np.array(test_df['bigwig_values'].tolist())\n",
    "y_test = test_df['log_TPM'].values\n",
    "\n",
    "# 标准化输入特征\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape 输入数据\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 构建 CNN 模型\n",
    "input_layer = Input(shape=(X_train_final.shape[1], 1))\n",
    "\n",
    "# 第一层卷积\n",
    "conv1 = Conv1D(64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "conv1 = Dropout(0.3)(conv1)\n",
    "\n",
    "# 第二层卷积\n",
    "conv2 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "conv2 = Dropout(0.3)(conv2)\n",
    "\n",
    "# 第三层卷积\n",
    "conv3 = Conv1D(256, kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "conv3 = Dropout(0.3)(conv3)\n",
    "\n",
    "# 平铺并添加全连接层\n",
    "flatten = Flatten()(conv3)\n",
    "dense = Dense(50, activation='relu')(flatten)\n",
    "output_layer = Dense(1)(dense)  # 输出层\n",
    "\n",
    "# 构建模型\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = Adam(learning_rate=0.00001)  # 更低的学习率\n",
    "model.compile(optimizer=optimizer, loss='mse')  # 使用均方误差作为损失函数\n",
    "\n",
    "# 训练模型，使用自定义回调\n",
    "custom_metrics = CustomMetrics(X_test_scaled, y_test)\n",
    "model.fit(X_train_final, y_train_final, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[custom_metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
